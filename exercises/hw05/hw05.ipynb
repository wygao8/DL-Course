{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "937a3ef6",
   "metadata": {},
   "source": [
    "## Homework Assignment 8 - Multilayer Perceptrons (MLP)\n",
    "\n",
    "We introduced some linear models to do some simple regression and classification tasks. \n",
    "If our labels truly were related to the input data by a simple affine transformation, \n",
    "then this approach would be sufficient.\n",
    "However, linearity (in affine transformations) is a *strong* assumption.\n",
    "\n",
    "Please implement the following functions and form a simple MLP!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6374a79",
   "metadata": {},
   "source": [
    "## Hidden Layers\n",
    "### Limitations of Linear Models\n",
    "\n",
    "For example, linearity implies the *weaker* assumption of *monotonicity*, i.e.,\n",
    "that any increase in our feature must\n",
    "either always cause an increase in our model's output\n",
    "(if the corresponding weight is positive),\n",
    "or always cause a decrease in our model's output\n",
    "(if the corresponding weight is negative).\n",
    "Sometimes that makes sense.\n",
    "For example, if we were trying to predict\n",
    "whether an individual will repay a loan,\n",
    "we might reasonably assume that all other things being equal,\n",
    "an applicant with a higher income\n",
    "would always be more likely to repay\n",
    "than one with a lower income.\n",
    "While monotonic, this relationship likely\n",
    "is not linearly associated with the probability of\n",
    "repayment. An increase in income from \\$0 to \\$50,000\n",
    "likely corresponds to a bigger increase\n",
    "in likelihood of repayment\n",
    "than an increase from \\$1 million to \\$1.05 million.\n",
    "One way to handle this might be to post-process our outcome\n",
    "such that linearity becomes more plausible,\n",
    "by using the logistic map (and thus the logarithm of the probability of outcome).\n",
    "\n",
    "Note that we can easily come up with examples\n",
    "that violate monotonicity.\n",
    "Say for example that we want to predict health as a function\n",
    "of body temperature.\n",
    "For individuals with a body temperature\n",
    "above 37째C (98.6째F),\n",
    "higher temperatures indicate greater risk.\n",
    "However, for individuals with body temperatures\n",
    "below 37째C, lower temperatures indicate greater risk!\n",
    "Again, we might resolve the problem\n",
    "with some clever preprocessing, such as using the distance from 37째C\n",
    "as a feature.\n",
    "\n",
    "But what about classifying images of cats and dogs?\n",
    "Should increasing the intensity\n",
    "of the pixel at location (13, 17)\n",
    "always increase (or always decrease)\n",
    "the likelihood that the image depicts a dog?\n",
    "Reliance on a linear model corresponds to the implicit\n",
    "assumption that the only requirement\n",
    "for differentiating cats vs. dogs is to assess\n",
    "the brightness of individual pixels.\n",
    "This approach is doomed to fail in a world\n",
    "where inverting an image preserves the category.\n",
    "\n",
    "And yet despite the apparent absurdity of linearity here,\n",
    "as compared with our previous examples,\n",
    "it is less obvious that we could address the problem\n",
    "with a simple preprocessing fix.\n",
    "That is, because the significance of any pixel\n",
    "depends in complex ways on its context\n",
    "(the values of the surrounding pixels).\n",
    "While there might exist a representation of our data\n",
    "that would take into account\n",
    "the relevant interactions among our features,\n",
    "on top of which a linear model would be suitable,\n",
    "we simply do not know how to calculate it by hand.\n",
    "With deep neural networks, we used observational data\n",
    "to jointly learn both a representation via hidden layers\n",
    "and a linear predictor that acts upon that representation.\n",
    "\n",
    "We can overcome the limitations of linear models\n",
    "by incorporating one or more hidden layers.\n",
    "The easiest way to do this is to stack\n",
    "many fully connected layers on top of each other.\n",
    "Each layer feeds into the layer above it,\n",
    "until we generate outputs.\n",
    "We can think of the first $L-1$ layers\n",
    "as our representation and the final layer\n",
    "as our linear predictor.\n",
    "This architecture is commonly called\n",
    "a *multilayer perceptron*,\n",
    "often abbreviated as *MLP*, as shown below.\n",
    "\n",
    "![An MLP with a hidden layer of 5 hidden units. ](./imgs/mlp.svg)\n",
    "\n",
    "This MLP has 4 inputs, 3 outputs,\n",
    "and its hidden layer contains 5 hidden units.\n",
    "Since the input layer does not involve any calculations,\n",
    "producing outputs with this network\n",
    "requires implementing the computations\n",
    "for both the hidden and output layers;\n",
    "thus, the number of layers in this MLP is 2.\n",
    "Note that both layers are fully connected.\n",
    "Every input influences every neuron in the hidden layer,\n",
    "and each of these in turn influences\n",
    "every neuron in the output layer. Alas, we are not quite\n",
    "done yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ce1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.datasets as dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e27f8b0e",
   "metadata": {},
   "source": [
    "### ReLU Function\n",
    "\n",
    "The most popular choice,\n",
    "due to both simplicity of implementation and\n",
    "its good performance on a variety of predictive tasks,\n",
    "is the *rectified linear unit* (*ReLU*).\n",
    "[**ReLU provides a very simple nonlinear transformation**].\n",
    "Given an element $x$, the function is defined\n",
    "as the maximum of that element and $0$:\n",
    "\n",
    "$$\\operatorname{ReLU}(x) = \\max(x, 0).$$\n",
    "\n",
    "Informally, the ReLU function retains only positive\n",
    "elements and discards all negative elements\n",
    "by setting the corresponding activations to 0.\n",
    "To gain some intuition, we can plot the function.\n",
    "As you can see, the activation function is piecewise linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce33b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: implement ReLU (Rectified Linear Unit) activation function\n",
    "def relu(x):\n",
    "    # to do\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af671e7f",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "[**The *sigmoid function* transforms its inputs**],\n",
    "for which values lie in the domain $\\mathbb{R}$,\n",
    "(**to outputs that lie on the interval (0, 1).**)\n",
    "For that reason, the sigmoid is\n",
    "often called a *squashing function*:\n",
    "it squashes any input in the range (-inf, inf)\n",
    "to some value in the range (0, 1):\n",
    "\n",
    "$$\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$\n",
    "\n",
    "In the earliest neural networks, scientists\n",
    "were interested in modeling biological neurons\n",
    "which either *fire* or *do not fire*.\n",
    "Thus the pioneers of this field,\n",
    "going all the way back to McCulloch and Pitts,\n",
    "the inventors of the artificial neuron,\n",
    "focused on thresholding units in 1943.\n",
    "A thresholding activation takes value 0\n",
    "when its input is below some threshold\n",
    "and value 1 when the input exceeds the threshold.\n",
    "\n",
    "When attention shifted to gradient based learning,\n",
    "the sigmoid function was a natural choice\n",
    "because it is a smooth, differentiable\n",
    "approximation to a thresholding unit.\n",
    "Sigmoids are still widely used as\n",
    "activation functions on the output units,\n",
    "when we want to interpret the outputs as probabilities\n",
    "for binary classification problems: you can think of the sigmoid as a special case of the softmax.\n",
    "However, the sigmoid has mostly been replaced\n",
    "by the simpler and more easily trainable ReLU\n",
    "for most use in hidden layers. Much of this has to do\n",
    "with the fact that the sigmoid poses challenges for optimization\n",
    "since its gradient vanishes for large positive *and* negative arguments.\n",
    "This can lead to plateaus that are difficult to escape from.\n",
    "\n",
    "Below, please implement a Sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9984eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: pls implement Sigmoid function\n",
    "def sigmoid(x):\n",
    "  # to do\n",
    "  pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "670e2364",
   "metadata": {},
   "source": [
    "### Tanh Function\n",
    "\n",
    "Like the sigmoid function, [**the tanh (hyperbolic tangent)\n",
    "function also squashes its inputs**],\n",
    "transforming them into elements on the interval (**between -1 and 1**):\n",
    "\n",
    "$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$\n",
    "\n",
    "We plot the tanh function below. Note that as input nears 0, the tanh function approaches a linear transformation. Although the shape of the function is similar to that of the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613532b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: pls implement Sigmoid function\n",
    "def tanh(x):\n",
    "    # to do\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ad85df1",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "Let's begin again by implementing a MLP from scratch.\n",
    "\n",
    "### Initializing Model Parameters\n",
    "\n",
    "We will use MNIST dataset (handwritten numbers), which contains 10 classes.\n",
    "Each image consists of a $28 \\times 28 = 784$\n",
    "grid of grayscale pixel values.\n",
    "As before we will disregard the spatial structure\n",
    "among the pixels for now,\n",
    "so we can think of this as a classification dataset\n",
    "with 784 input features and 10 classes.\n",
    "To begin, we will [**implement an MLP\n",
    "with one hidden layer and 256 hidden units.**]\n",
    "Both the number of layers and their width are adjustable\n",
    "(they are considered hyperparameters).\n",
    "Typically, we choose the layer widths to be divisible by larger powers of 2.\n",
    "This is computationally efficient due to the way\n",
    "memory is allocated and addressed in hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133133ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_outputs, num_hiddens):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Tips: try to use nn.Sequential to implement two layer MLP with ReLU as activation function\n",
    "        # to do\n",
    "        pass\n",
    "\n",
    "    # forward function processes data througth model and return results\n",
    "    def forward(self, x):\n",
    "        # to do\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71c01e5c",
   "metadata": {},
   "source": [
    "Previously, we defined `forward` methods for models to transform input using the model parameters.\n",
    "These operations are essentially a pipeline:\n",
    "you take an input and\n",
    "apply a transformation (e.g.,\n",
    "matrix multiplication with weights followed by bias addition),\n",
    "then repetitively use the output of the current transformation as\n",
    "input to the next transformation.\n",
    "However, you may have noticed that \n",
    "no `forward` method is defined here.\n",
    "In fact, `MLP` inherits the `forward` method from the `Module` class to \n",
    "simply invoke `self.net(X)` (`X` is input),\n",
    "which is now defined as a sequence of transformations\n",
    "via the `Sequential` class.\n",
    "The `Sequential` class abstracts the forward process\n",
    "enabling us to focus on the transformations.\n",
    "We will further discuss how the `Sequential` class works in :numref.\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "[**The training loop**] is exactly the same\n",
    "as when we implemented softmax regression.\n",
    "This modularity enables us to separate\n",
    "matters concerning the model architecture\n",
    "from orthogonal considerations.\n",
    "\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb42e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Prepare for training\n",
    "X_train = Variable(torch.from_numpy(X_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test)).long()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b9570f2",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax1.plot(X_plot[:, 0], X_plot[:, 1], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.axis('equal')\n",
    "ax1.legend()\n",
    "\n",
    "for target, target_name in enumerate(names):\n",
    "    X_plot = X[y == target]\n",
    "    ax2.plot(X_plot[:, 2], X_plot[:, 3], \n",
    "             linestyle='none', \n",
    "             marker='o', \n",
    "             label=target_name)\n",
    "ax2.set_xlabel(feature_names[2])\n",
    "ax2.set_ylabel(feature_names[3])\n",
    "ax2.axis('equal')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7471cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fixed random number seed and hyperparameters\n",
    "torch.manual_seed(2023)\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP(num_hiddens=1024, num_outputs=10)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=learning_rate) \n",
    "\n",
    "# Run the training loop\n",
    "loss_list, acc_list = [], []\n",
    "for epoch in range(0, epochs): # 5 epochs at maximum\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = mlp(X_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(outputs, y_train)\n",
    "    \n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "        \n",
    "    # Print statistics\n",
    "    loss_list.append(loss.item())\n",
    "    acc_list.append(torch.sum(torch.argmax(outputs, axis=1) == y_train) / len(y_train))\n",
    "\n",
    "    print('Training loss: %.3f, Training accuracy: %.3f' %\n",
    "        (loss_list[epoch], acc_list[epoch] * 100))\n",
    "\n",
    "    # test\n",
    "    current_loss = 0.0\n",
    "    current_acc = 0.0\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = mlp(X_test)\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        current_acc += torch.sum(torch.argmax(outputs, axis=1) == y_test) / len(y_test)\n",
    "\n",
    "\n",
    "        print('Test loss: %.3f, Test accuracy: %.3f' %\n",
    "            (current_loss, current_acc * 100))\n",
    "\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83c33483",
   "metadata": {},
   "source": [
    "### Plot Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6600a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "ax1.plot(acc_list)\n",
    "ax1.set_ylabel(\"training accuracy\")\n",
    "ax2.plot(loss_list)\n",
    "ax2.set_ylabel(\"loss\")\n",
    "ax2.set_xlabel(\"epochs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "917b7ec3c84293b92ddb6ccd238cf6b108767bfe42396bb9d40df41deeda18cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
